{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6993952c-61ec-42cf-803d-562b85777b64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"USE CATALOG yogurt\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS quality\")\n",
    "spark.sql(\"USE SCHEMA quality\")\n",
    "\n",
    "display(spark.sql(\"SELECT current_catalog() AS catalog, current_database() AS schema\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22297718-38b8-4340-8cae-6e81edecd202",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS dq_pk_config (\n",
    "  table_name STRING,\n",
    "  pk_cols    STRING      \n",
    ") USING DELTA;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS dq_fk_config (\n",
    "  child_table  STRING,\n",
    "  child_cols   STRING,   \n",
    "  parent_table STRING,\n",
    "  parent_cols  STRING    \n",
    ") USING DELTA;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS dq_runs (\n",
    "  run_id     BIGINT,\n",
    "  run_ts     TIMESTAMP,\n",
    "  triggered_by STRING\n",
    ") USING DELTA;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS dq_checks (\n",
    "  run_id      BIGINT,\n",
    "  layer       STRING,        \n",
    "  table_name  STRING,\n",
    "  check_name  STRING,\n",
    "  status      STRING,        \n",
    "  measured_value STRING,\n",
    "  expected_expr  STRING,\n",
    "  details     STRING,\n",
    "  checked_ts  TIMESTAMP\n",
    ") USING DELTA;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS dq_table_metrics (\n",
    "  run_id     BIGINT,\n",
    "  layer      STRING,         \n",
    "  table_name STRING,\n",
    "  row_count  BIGINT,\n",
    "  max_date   DATE,           \n",
    "  collected_ts TIMESTAMP\n",
    ") USING DELTA;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f4d1b66-8360-43ef-ac95-42a5c9b1f47e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.sql(\"TRUNCATE TABLE quality.dq_pk_config\")\n",
    "spark.sql(\"\"\"INSERT INTO quality.dq_pk_config VALUES\n",
    "  ('region'  ,'r_regionkey'),\n",
    "  ('nation'  ,'n_nationkey'),\n",
    "  ('part'    ,'p_partkey'),\n",
    "  ('supplier','s_suppkey'),\n",
    "  ('partsupp','ps_partkey,ps_suppkey'),\n",
    "  ('customer','c_custkey'),\n",
    "  ('orders'  ,'o_orderkey'),\n",
    "  ('lineitem','l_orderkey,l_linenumber')\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"TRUNCATE TABLE quality.dq_fk_config\")\n",
    "spark.sql(\"\"\"INSERT INTO quality.dq_fk_config VALUES\n",
    "  ('nation'  ,'n_regionkey'      ,'region'  ,'r_regionkey'),\n",
    "  ('supplier','s_nationkey'      ,'nation'  ,'n_nationkey'),\n",
    "  ('customer','c_nationkey'      ,'nation'  ,'n_nationkey'),\n",
    "  ('orders'  ,'o_custkey'        ,'customer','c_custkey'),\n",
    "  ('lineitem','l_orderkey'       ,'orders'  ,'o_orderkey'),\n",
    "  ('lineitem','l_partkey'        ,'part'    ,'p_partkey'),\n",
    "  ('lineitem','l_suppkey'        ,'supplier','s_suppkey'),\n",
    "  ('partsupp','ps_partkey'       ,'part'    ,'p_partkey'),\n",
    "  ('partsupp','ps_suppkey'       ,'supplier','s_suppkey')\n",
    "\"\"\")\n",
    "print(\"Config loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23f66bee-52e6-4a4f-941d-51245d2d6e67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "run_id = int(time.time())\n",
    "spark.sql(f\"INSERT INTO quality.dq_runs VALUES ({run_id}, current_timestamp(), 'Person C')\")\n",
    "\n",
    "bronze = \"yogurt.bronze\"\n",
    "silver = \"yogurt.silver\"\n",
    "\n",
    "tables = [\"region\",\"nation\",\"part\",\"supplier\",\"partsupp\",\"customer\",\"orders\",\"lineitem\"]\n",
    "\n",
    "for t in tables:\n",
    "    bc = spark.sql(f\"SELECT COUNT(*) c FROM {bronze}.{t}\").collect()[0][0]\n",
    "    sc = spark.sql(f\"SELECT COUNT(*) c FROM {silver}.{t}\").collect()[0][0]\n",
    "    status = \"PASS\" if bc == sc else \"FAIL\"\n",
    "    spark.sql(f\"\"\"\n",
    "      INSERT INTO quality.dq_checks\n",
    "      VALUES ({run_id}, 'silver', '{t}', 'row_count_matches_bronze', '{status}',\n",
    "              '{sc}', 'silver_count == bronze_count', 'bronze={bc}, silver={sc}', current_timestamp())\n",
    "    \"\"\")\n",
    "\n",
    "pk_cfg = {r['table_name']: r['pk_cols'] for r in spark.table(\"quality.dq_pk_config\").collect()}\n",
    "\n",
    "for t, pk in pk_cfg.items():\n",
    "    cols = [c.strip() for c in pk.split(\",\")]\n",
    "    null_pred = \" OR \".join([f\"{c} IS NULL\" for c in cols])\n",
    "    nulls = spark.sql(f\"SELECT COUNT(*) c FROM {silver}.{t} WHERE {null_pred}\").collect()[0][0]\n",
    "    status = \"PASS\" if nulls == 0 else \"FAIL\"\n",
    "    spark.sql(f\"\"\"\n",
    "      INSERT INTO quality.dq_checks\n",
    "      VALUES ({run_id}, 'silver', '{t}', 'pk_not_null', '{status}',\n",
    "              '{nulls}', '{pk} IS NOT NULL', 'null_pk={nulls}', current_timestamp())\n",
    "    \"\"\")\n",
    "    group_cols = \", \".join(cols)\n",
    "    dup = spark.sql(f\"\"\"\n",
    "      SELECT COUNT(*) c FROM (\n",
    "        SELECT {group_cols}, COUNT(*) cnt\n",
    "        FROM {silver}.{t}\n",
    "        GROUP BY {group_cols}\n",
    "        HAVING COUNT(*) > 1\n",
    "      )\n",
    "    \"\"\").collect()[0][0]\n",
    "    status = \"PASS\" if dup == 0 else \"FAIL\"\n",
    "    spark.sql(f\"\"\"\n",
    "      INSERT INTO quality.dq_checks\n",
    "      VALUES ({run_id}, 'silver', '{t}', 'pk_unique', '{status}',\n",
    "              '{dup}', 'COUNT(GROUP BY {pk}) <= 1', 'duplicate_pk_groups={dup}', current_timestamp())\n",
    "    \"\"\")\n",
    "\n",
    "fk_rows = spark.table(\"quality.dq_fk_config\").collect()\n",
    "for r in fk_rows:\n",
    "    child, child_cols, parent, parent_cols = r['child_table'], r['child_cols'], r['parent_table'], r['parent_cols']\n",
    "    c_cols = [c.strip() for c in child_cols.split(\",\")]\n",
    "    p_cols = [c.strip() for c in parent_cols.split(\",\")]\n",
    "    on_cond = \" AND \".join([f\"c.{cc} = p.{pp}\" for cc, pp in zip(c_cols, p_cols)])\n",
    "    orphans = spark.sql(f\"\"\"\n",
    "      SELECT COUNT(*) c FROM {silver}.{child} c\n",
    "      LEFT ANTI JOIN {silver}.{parent} p ON {on_cond}\n",
    "    \"\"\").collect()[0][0]\n",
    "    status = \"PASS\" if orphans == 0 else \"FAIL\"\n",
    "    spark.sql(f\"\"\"\n",
    "      INSERT INTO quality.dq_checks\n",
    "      VALUES ({run_id}, 'silver', '{child}', 'fk_{child}_{parent}', '{status}',\n",
    "              '{orphans}', 'no orphans vs {parent}', 'orphans={orphans}', current_timestamp())\n",
    "    \"\"\")\n",
    "\n",
    "date_cols = {'orders':'o_orderdate', 'lineitem':'l_shipdate'}\n",
    "for t in tables:\n",
    "    cnt = spark.sql(f\"SELECT COUNT(*) c FROM {silver}.{t}\").collect()[0][0]\n",
    "    if t in date_cols:\n",
    "        mx = spark.sql(f\"SELECT MAX({date_cols[t]}) d FROM {silver}.{t}\").collect()[0][0]\n",
    "        mx_val = f\"DATE('{mx}')\" if mx is not None else \"NULL\"\n",
    "        spark.sql(f\"INSERT INTO quality.dq_table_metrics VALUES ({run_id}, 'silver', '{t}', {cnt}, {mx_val}, current_timestamp())\")\n",
    "    else:\n",
    "        spark.sql(f\"INSERT INTO quality.dq_table_metrics VALUES ({run_id}, 'silver', '{t}', {cnt}, NULL, current_timestamp())\")\n",
    "\n",
    "print(f\"Completed DQ run {run_id}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec3bb226-f48e-4c4e-b1a9-444ad25d85ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE VIEW vw_dq_latest_run AS\n",
    "SELECT run_id\n",
    "FROM quality.dq_runs\n",
    "ORDER BY run_ts DESC\n",
    "LIMIT 1;\n",
    "\n",
    "CREATE OR REPLACE VIEW vw_dq_checks_latest AS\n",
    "SELECT c.*\n",
    "FROM quality.dq_checks c\n",
    "JOIN vw_dq_latest_run r ON c.run_id = r.run_id;\n",
    "\n",
    "CREATE OR REPLACE VIEW vw_dq_failures_latest AS\n",
    "SELECT *\n",
    "FROM vw_dq_checks_latest\n",
    "WHERE status = 'FAIL';\n",
    "\n",
    "CREATE OR REPLACE VIEW vw_dq_summary_latest AS\n",
    "SELECT table_name,\n",
    "       SUM(CASE WHEN status='PASS' THEN 1 ELSE 0 END) AS pass_cnt,\n",
    "       SUM(CASE WHEN status='FAIL' THEN 1 ELSE 0 END) AS fail_cnt\n",
    "FROM vw_dq_checks_latest\n",
    "GROUP BY table_name\n",
    "ORDER BY fail_cnt DESC, table_name;\n",
    "\n",
    "CREATE OR REPLACE VIEW vw_dq_kpi_trend AS\n",
    "SELECT run_id,\n",
    "       SUM(CASE WHEN status='PASS' THEN 1 ELSE 0 END) AS passes,\n",
    "       SUM(CASE WHEN status='FAIL' THEN 1 ELSE 0 END) AS fails,\n",
    "       ROUND(100.0 * SUM(CASE WHEN status='PASS' THEN 1 ELSE 0 END) / COUNT(*), 2) AS pass_rate_pct,\n",
    "       MAX(r.run_ts) AS run_ts\n",
    "FROM quality.dq_checks c\n",
    "JOIN quality.dq_runs r USING(run_id)\n",
    "GROUP BY run_id\n",
    "ORDER BY run_ts DESC;\n",
    "\n",
    "CREATE OR REPLACE VIEW vw_rowcount_trend AS\n",
    "SELECT run_id, table_name, row_count, max_date, collected_ts\n",
    "FROM quality.dq_table_metrics\n",
    "ORDER BY collected_ts DESC, table_name;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "448ac0d8-1766-4f04-89d1-01d1e0861fc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM vw_dq_summary_latest;\n",
    "\n",
    "SELECT * FROM vw_dq_failures_latest;\n",
    "\n",
    "SELECT * FROM vw_dq_kpi_trend ORDER BY run_ts DESC LIMIT 10;\n",
    "\n",
    "SELECT * FROM vw_rowcount_trend ORDER BY collected_ts DESC LIMIT 80;\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5425104935460502,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Quality Checks Monitoring",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
